<!DOCTYPE HTML>
<html lang="en">
<meta charset="UTF-8">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xin Sun's Homepage</title>

  <meta name="author" content="Xin Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <strong><name>Xin Sun (孙新)</name></strong>
                  </p>

                  <p>Currently, I’m a third-year Ph.D. student in <a href="https://cmic.sjtu.edu.cn/cn/Default.aspx"> Cooperative
                   Medianet Innovation Center</a> at <a href="http://en.sjtu.edu.cn/"> Shanghai Jiao Tong University</a>, supervised by 
                   <a href="https://baike.baidu.com/item/%E5%91%A8%E6%9B%A6/16034807"> Xi Zhou</a>.
                   Prior to that, I received my bachelor’s degree from <a href="http://dice.xjtu.edu.cn/"> Information 
                   Engineering</a> at <a href="http://en.xjtu.edu.cn/"> Xi’an Jiao Tong University</a> in 2019.
                   </p>

                  <p>My research interests include but are not limited to Computer Vision, Natural Language Processing, and <strong><em>Cross-modal Learning </em></strong>.
                    I am now focusing on <strong><em>Temporal Sentence Grounding in Videos (TSGV)</em></strong>, a.k.a., Natural Language Video 
                    Localization (NLVL) or Video Moment Retrieval (VMR).

                  </p>


                  <p style="text-align:center">
                    <a href="mailto:huntersx@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=-UL00sEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/Huntersxsx"> GitHub </a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/xin-sun-96547b220/"> LinkedIn </a> &nbsp/&nbsp
                    <a href="#"> CV </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/crayon.png"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>News</heading></strong>
            </td>
          </table>

          <ul>
            <li><span style="color: gray" size="6px">[2022.05]</span>
              &#127881 One paper was accepted by <strong><em>TCSVT</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2022.03]</span>
              &#127881 One paper was accepted by <strong><em>SIGIR 2022</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2021.09]</span>
              &#127881 One paper was accepted by <strong><em>EMNLP 2021</em></strong>.
            </li>


          </ul>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Publications</heading></strong>
            </td>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="w2w_stop()" onmouseover="w2w_start()">
                <td style="padding:25px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='w2w_image'>
                      <img src='images/which_to_where_motivation.png' width="200">
                    </div>
                    <img src='images/which_to_where_framework.png' width="200">
                  </div>
                  <script type="text/javascript">
                    function w2w_start() {
                      document.getElementById('w2w_image').style.opacity = "1";
                    }

                    function w2w_stop() {
                      document.getElementById('w2w_image').style.opacity = "0";
                    }
                    w2w_stop()
                  </script>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                      <strong><papertitle>Efficient Video Grounding with Which-Where Reading Comprehension </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  <a href="#">Xi Zhou</a>,
                  <a href="https://imsg.ac.cn/people/geshiming.html">Shiming Ge</a>,
                  &nbsp <em>in TCSVT</em>  
                  <br>
                  <a href="#">paper</a>
                  /
                  <a href="#">code</a>
                  <p></p>
                  <p>
                    We present an efficient framework in a fashion <strong>from which to where</strong> to facilitate video grounding. The core idea is imitating
                    the reading comprehension process to gradually narrow the decision space. The “which” step first roughly selects a candidate area by evaluating 
                    which video segment in the predefined set is closest to the ground truth, while the “where” step aims to precisely regress the temporal boundary of 
                    the selected video segment from the shrunk decision space. Extensive experiments demonstrate the effectiveness of our framework.
                    </p>
                </td>
              </tr>


              <tr onmouseout="MGPN_stop()" onmouseover="MGPN_start()">
                <td style="padding:25px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='MGPN_image'>
                      <img src='images/MGPN_motivation.png' width="200">
                    </div>
                    <img src='images/MGPN_framework.png' width="200">
                  </div>
                  <script type="text/javascript">
                    function MGPN_start() {
                      document.getElementById('MGPN_image').style.opacity = "1";
                    }

                    function MGPN_stop() {
                      document.getElementById('MGPN_image').style.opacity = "0";
                    }
                    MGPN_stop()
                  </script>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                      <strong><papertitle>You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos </papertitle></strong>
                  <br>
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=SmlaHBYAAAAJ">Xuan Wang</a>,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=cH-m1i8AAAAJ">Qiong Liu</a>,
                  <a href="#">Xi Zhou</a>,
                  &nbsp <em>in SIGIR 2022</em>  
                  <br>
                  <a href="#">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/MGPN">code</a>
                  <p></p>
                  <p>
                    We formulate moment retrieval task from the perspective of multi-choice reading comprehension and propose a novel 
                    <strong>M</strong>ulti-<strong>G</strong>ranularity <strong>P</strong>erception <strong>N</strong>etwork (<strong>MGPN</strong>) to tackle it. We integrate several human reading strategies 
                    (i.e. passage question reread, enhanced passage question alignment, choice comparison) into our framework 
                    and empower our model to perceive intra-modality and inter-modality information at a multi-granularity level. 
                    Extensive experiments demonstrate the effectiveness and efficiency of our proposed MGPN.
                    </p>
                </td>
              </tr>


              <tr onmouseout="RaNet_stop()" onmouseover="RaNet_start()">
                <td style="padding:25px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='RaNet_image'>
                      <img src='images/RaNet_motivation.png' width="200">
                    </div>
                    <img src='images/RaNet_framework.png' width="200">
                  </div>
                  <script type="text/javascript">
                    function RaNet_start() {
                      document.getElementById('RaNet_image').style.opacity = "1";
                    }

                    function RaNet_stop() {
                      document.getElementById('RaNet_image').style.opacity = "0";
                    }
                    RaNet_stop()
                  </script>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                    <strong><papertitle>Relation-aware Video Reading Comprehension for Temporal Language Grounding </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>*,
                  <span style="color: red"><strong>Xin Sun</strong></span>*,
                  <a href="https://scholar.google.com/citations?user=be_ox9QAAAAJ&hl=en">MengMeng Xu</a>,
                  <a href="#">Xi Zhou</a>,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  &nbsp <em>in EMNLP 2021</em>  
                  <br>
                  (* means equal contribution)
                  <br>
                  <a href="https://aclanthology.org/2021.emnlp-main.324.pdf">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/RaNet">code</a>
                  <p></p>
                  <p>
                    We propose a novel <strong>R</strong>elation-<strong>a</strong>ware <strong>Net</strong>work (<strong>RaNet</strong>) 
                    to address the problem of temporal language grounding in videos. We propose to interact the visual and textual modalities 
                    in a coarse-and-fine fashion for token-aware and sentence-aware representation of each choice. 
                    Further, a GAT layer is introduced to mine the exhaustive relations between multi-choices for better ranking. 
                    Our model is efficient and outperforms the SOTA methods.
                  </p>
                </td>
              </tr>

          </table>

<!--           <p align="right">
            <font size="2">
              <a href="https://jonbarron.info/"> website template </a>
            </font>
          </p>
 -->

        </td>
      </tr>


  </table>
</body>

</html>