<!DOCTYPE HTML>
<html lang="en">
<meta charset="UTF-8">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xin Sun's Homepage</title>

  <meta name="author" content="Xin Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xin Sun | 孙新 </name>
                  </p>

                  <p>Currently, I’m a forth-year Ph.D. student in <a href="https://cmic.sjtu.edu.cn/cn/Default.aspx"> Cooperative
                   Medianet Innovation Center</a> at <a href="http://en.sjtu.edu.cn/"> Shanghai Jiao Tong University</a> (SJTU), supervised by 
                   <a href="https://baike.baidu.com/item/%E5%91%A8%E6%9B%A6/16034807"> Xi Zhou</a>.
                   Prior to that, I received my bachelor’s degree from <a href="http://dice.xjtu.edu.cn/"> Information 
                   Engineering</a> at <a href="http://en.xjtu.edu.cn/"> Xi’an Jiao Tong University</a> (XJTU) in 2019.
                   </p>

                  <p>My research interests include but are not limited to Computer Vision, Natural Language Processing, and <strong><em>Cross-modal Learning </em></strong>.
                  I am now focusing on <strong><em>Video Moment Retrieval (VMR)</em></strong>, <strong><em>Referring Image Segmentation (RIS)</em></strong>. I am also interested in <strong><em>Audio-Visual Video Parsing (AVVP)</em></strong>, <strong><em>Vision and Language Tracking (VLT)</em></strong>.

                  </p>


                  <p style="text-align:center">
                    <a href="mailto:huntersx@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=-UL00sEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/Huntersxsx"> GitHub </a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/xin-sun-96547b220/"> LinkedIn </a> &nbsp/&nbsp
                    <a href="#"> CV </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/crayon.png"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>News</heading></strong>
            </td>
          </table>

          <ul>
            <li><span style="color: gray" size="6px">[2023.06]</span>
              &#127881 One paper was accepted by <strong><em>ICANN 2023</em></strong>.
            </li> 
            <li><span style="color: gray" size="6px">[2023.02]</span>
              &#127881 One paper was accepted by <strong><em>TCSVT 2023</em></strong>.
            </li>          	
            <li><span style="color: gray" size="6px">[2022.05]</span>
              &#127881 One paper was accepted by <strong><em>TCSVT 2022</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2022.03]</span>
              &#127881 One paper was accepted by <strong><em>SIGIR 2022</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2021.09]</span>
              &#127881 One paper was accepted by <strong><em>EMNLP 2021</em></strong>.
            </li>


          </ul>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Publications</heading></strong>
            </td>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
                    <img src='images/ICANN.png' width="200">
                  </div>
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Exploiting Multi-modal Fusion for Robust Face Representation Learning with Missing Modality </papertitle></strong>
                  <br>
                  Yizhe Zhu,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  Xi Zhou,
                  <br>
                  <a href="#">paper</a>
                  <br>
                  in <em>International Conference on Artificial Neural Networks (<strong>ICANN</strong>)</em>, 2023. (<strong>CCF-C</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We propose a multi-modal fusion framework that addresses the problem of uncertain missing modalities in face recognition.
                    Specifically, we first introduce a novel modality-missing loss function based on triplet hard loss to learn individual features for RGB, depth, and thermal modalities. 
                    We then use a central moment discrepancy (CMD) based distance constraint training strategy to learn joint modality-invariant representations. 
                    This approach fully leverages the characteristics of heterogeneous modalities to mitigate the modality gap, resulting in robust multi-modal joint representations.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="CRNet_stop()" onmouseover="CRNet_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='CRNet_image'>
                      <img src='images/CRNet_motivation.png' width="200">
                    </div> -->
                    <img src='images/CRNet_framework.png' width="200">
                  </div>
<!--                   <script type="text/javascript">
                    function CRNet_start() {
                      document.getElementById('CRNet_image').style.opacity = "1";
                    }

                    function CRNet_stop() {
                      document.getElementById('CRNet_image').style.opacity = "0";
                    }
                    CRNet_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Video Moment Retrieval via Comprehensive Relation-aware Network </papertitle></strong>
                  <br>
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
				          <!-- <a href="#">Yizhe Zhu</a>, -->
                  Yizhe Zhu,
                  <!-- <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=SmlaHBYAAAAJ">Xuan Wang</a>, -->
                  Xuan Wang,
                  Xi Zhou,
                  <!-- &nbsp <em>in TCSVT 2023</em>   -->
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10056330">paper</a>
                  <br>
                  in <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2023. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                  	This manuscript is a substantial <strong>extension of our RaNet in EMNLP2021</strong> with several improvements (<em>i.e.</em> background suppression module, clip-level interaction, and IoU attention mechanism). 
                  	<!--  First, a background suppression module is introduced to reduce the impact of unrelated background clips. 
                  	Second, we model the interactions between clips and sentences, as well as clips and words, to comprehensively capture inter-modality relations. 
                  	Finally, we propose a novel IoU attention mechanism to focus more on the dependencies among highly-correlated video moments. -->
                  	The background suppression module and IoU attention mechanism work together to enhance the hierarchical relations within the model by respectively modulating clip-level and moment-level features. 
                  	The clip-level interaction provides a complementary perspective by capturing localized visual information, resulting in a multi-granular perception of inter-modality information. 
                  	As a result, the harmonious integration of these modules significantly improves the model's ability to model comprehensive relations.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="w2w_stop()" onmouseover="w2w_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='w2w_image'>
                      <img src='images/which_to_where_motivation.png' width="200">
                    </div> -->
                    <img src='images/which_to_where_framework.png' width="200">
                  </div>
<!--                   <script type="text/javascript">
                    function w2w_start() {
                      document.getElementById('w2w_image').style.opacity = "1";
                    }

                    function w2w_stop() {
                      document.getElementById('w2w_image').style.opacity = "0";
                    }
                    w2w_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Efficient Video Grounding with Which-Where Reading Comprehension </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  Xi Zhou,
                  <a href="https://imsg.ac.cn/people/geshiming.html">Shiming Ge</a>,
                  <!-- &nbsp <em>in TCSVT 202</em>   -->
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9771472">paper</a>
                  <br>
                  in <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2022. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We present an efficient framework in a fashion <strong>from which to where</strong> to facilitate video grounding. The core idea is imitating
                    the reading comprehension process to gradually narrow the decision space. The “which” step first roughly selects a candidate area by evaluating 
                    which video segment in the predefined set is closest to the ground truth, while the “where” step aims to precisely regress the temporal boundary of 
                    the selected video segment from the shrunk decision space. Extensive experiments demonstrate the effectiveness of our framework.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="MGPN_stop()" onmouseover="MGPN_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='MGPN_image'>
                      <img src='images/MGPN_motivation.png' width="200">
                    </div> -->
                    <img src='images/MGPN_framework.png' width="200">
                  </div>
<!--                   <script type="text/javascript">
                    function MGPN_start() {
                      document.getElementById('MGPN_image').style.opacity = "1";
                    }

                    function MGPN_stop() {
                      document.getElementById('MGPN_image').style.opacity = "0";
                    }
                    MGPN_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos </papertitle></strong>
                  <br>
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  Xuan Wang,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=cH-m1i8AAAAJ">Qiong Liu</a>,
                  Xi Zhou,
                  <!-- &nbsp <em>in SIGIR 2022</em>   -->
                  <br>
                  <a href="https://arxiv.org/abs/2205.12886">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/MGPN">code</a>
                  <br>
                  in <em>International ACM SIGIR Conference on Research and Development in Information Retrieval (<strong>SIGIR</strong>)</em>, 2022. (<strong>CCF-A</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We formulate moment retrieval task from the perspective of multi-choice reading comprehension and propose a novel 
                    <strong>M</strong>ulti-<strong>G</strong>ranularity <strong>P</strong>erception <strong>N</strong>etwork (<strong>MGPN</strong>) to tackle it. We integrate several human reading strategies 
                    (i.e. passage question reread, enhanced passage question alignment, choice comparison) into our framework 
                    and empower our model to perceive intra-modality and inter-modality information at a multi-granularity level. 
                    Extensive experiments demonstrate the effectiveness and efficiency of our proposed MGPN.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="RaNet_stop()" onmouseover="RaNet_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='RaNet_image'>
                      <img src='images/RaNet_motivation.png' width="200">
                    </div> -->
                    <img src='images/RaNet_framework.png' width="200">
                  </div>
<!--                   <script type="text/javascript">
                    function RaNet_start() {
                      document.getElementById('RaNet_image').style.opacity = "1";
                    }

                    function RaNet_stop() {
                      document.getElementById('RaNet_image').style.opacity = "0";
                    }
                    RaNet_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                    <strong><papertitle>Relation-aware Video Reading Comprehension for Temporal Language Grounding </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>*,
                  <span style="color: red"><strong>Xin Sun</strong></span>*,
                  <a href="https://scholar.google.com/citations?user=be_ox9QAAAAJ&hl=en">MengMeng Xu</a>,
                  Xi Zhou,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  <!-- &nbsp <em>in EMNLP 2021</em>   -->
                  <br>
                  (* means equal contribution)
                  <br>
                  <a href="https://aclanthology.org/2021.emnlp-main.324.pdf">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/RaNet">code</a>
                  <br>
                  in <em>Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>)</em>, 2021. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We propose a novel <strong>R</strong>elation-<strong>a</strong>ware <strong>Net</strong>work (<strong>RaNet</strong>) 
                    to address the problem of temporal language grounding in videos. We propose to interact the visual and textual modalities 
                    in a coarse-and-fine fashion for token-aware and sentence-aware representation of each choice. 
                    Further, a GAT layer is introduced to mine the exhaustive relations between multi-choices for better ranking. 
                    Our model is efficient and outperforms the SOTA methods.
                  </details>
                </td>
              </tr>

          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Professional Services</heading></strong>
            </td>
          </table>

        <ul>
          <li>
            <strong>Conference Reviewer:</strong> ACM MM'23
          </li>

        </ul>

        <br />
        <br />
         <a href="https://visitorbadge.io/status?path=https%3A%2F%2Fhuntersxsx.github.io%2F"><img src="https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fhuntersxsx.github.io%2F&countColor=%23dce775&style=flat" /></a>

        </td>
      </tr>


  </table>
</body>

</html>