<!DOCTYPE HTML>
<html lang="en">
<meta charset="UTF-8">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xin Sun's Homepage</title>

  <meta name="author" content="Xin Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

  <style>
  .enlarged-img {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: rgba(0, 0, 0, 0.5);
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 9999;
  }
  
  .enlarged-img img {
    max-width: 90%;
    max-height: 90%;
  }

      body {
      margin: 0; /* 去除默认的 body margin */
      font-family: Arial, sans-serif; /* 设置字体 */
    }


  nav {
      display: flex; /* 使用 flex 布局 */
      justify-content: center; /* 在主轴上居中对齐 */
      background-color: white; /* 导航栏背景色 */
      overflow: hidden;
    }

    nav a {
      float: left; /* 将链接横向排列 */
      display: block; /* 让链接占满父容器的宽度 */
      color: black; /* 链接文本颜色 */
      text-align: center; /* 文本居中 */
      padding: 14px 16px; /* 设置内边距 */
      text-decoration: none; /* 去掉下划线 */
      font-size: 20px; /* 设置字体大小 */
      font-weight: bold; /* 设置字体粗细 */
    }

    nav a:hover {
      background-color: #ddd; /* 鼠标悬停时的背景色 */
      color: orange; /* 鼠标悬停时的文本颜色 */
    }

</style>


<body>

  <nav>
  <a href="#news">News</a>
  <a href="#publications">Publications</a>
  <a href="#internship">Internship</a>
  <a href="#edution">Eduction</a>
  <a href="#awards">Awards</a>
  <a href="#services">Services</a>
</nav>

  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xin Sun | 孙新 </name>
                  </p>
                  <p style="text-align:center">
                    huntersx@sjtu.edu.cn
                  </p>

                  <p>Currently, I’m a fifth-year Ph.D. student in <a href="https://cmic.sjtu.edu.cn/cn/Default.aspx"> Cooperative
                   Medianet Innovation Center</a> at <a href="http://en.sjtu.edu.cn/"> Shanghai Jiao Tong University</a> (SJTU), supervised by 
                   <a href="https://baike.baidu.com/item/%E5%91%A8%E6%9B%A6/16034807"> Xi Zhou</a>.
                   Prior to that, I received my bachelor’s degree from <a href="http://dice.xjtu.edu.cn/"> Information 
                   Engineering</a> at <a href="http://en.xjtu.edu.cn/"> Xi’an Jiao Tong University</a> (XJTU) in 2019.
                   </p>

                  <p>My research interests include but are not limited to Computer Vision, Natural Language Processing, and <strong><em>Cross-modal Learning </em></strong>.
                  I am now focusing on <strong><em>Video Moment Retrieval (VMR)</em></strong>, <strong><em>Referring Image Segmentation (RIS)</em></strong>. I am also interested in <strong><em>Audio-Visual Video Parsing (AVVP)</em></strong>, <strong><em>Vision and Language Tracking (VLT)</em></strong>.

                  </p>


                  <p style="text-align:center">
                    <a href="mailto:huntersx@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=-UL00sEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/Huntersxsx"> GitHub </a> &nbsp/&nbsp
                    <!-- <a href="https://www.linkedin.com/in/xin-sun-96547b220/"> LinkedIn </a> &nbsp/&nbsp -->
                    <a href='https://orcid.org/my-orcid?orcid=0000-0002-3092-0583'> ORCID </a> &nbsp/&nbsp
                    <a href="#"> CV </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/crayon.png"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

        <section id="news">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>News</heading></strong>
            </td>
          </table>
        </section>

          <style>
              .scrolling-list {
                max-height: 120px; /* 设置最大高度，超过此高度将显示滚动条 */
                overflow-y: auto; /* 只在需要时显示垂直滚动条 */
                overflow-x: hidden; /* 隐藏水平滚动条 */
              }
          </style>

          <div class="scrolling-list">
          <ul>
            <li><span style="color: gray" size="6px">[2023.10]</span>
              &#127881 One paper was accepted by <strong><em>Neurocomputing 2023</em></strong>.
            </li> 
            <li><span style="color: gray" size="6px">[2023.07]</span>
              &#127881 One paper was accepted by <strong><em>ACM MM 2023</em></strong>.
            </li> 
            <li><span style="color: gray" size="6px">[2023.06]</span>
              &#127881 One paper was accepted by <strong><em>ICANN 2023</em></strong>.
            </li> 
            <li><span style="color: gray" size="6px">[2023.02]</span>
              &#127881 One paper was accepted by <strong><em>TCSVT 2023</em></strong>.
            </li>          	
            <li><span style="color: gray" size="6px">[2022.05]</span>
              &#127881 One paper was accepted by <strong><em>TCSVT 2022</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2022.03]</span>
              &#127881 One paper was accepted by <strong><em>SIGIR 2022</em></strong>.
            </li>
            <li><span style="color: gray" size="6px">[2021.09]</span>
              &#127881 One paper was accepted by <strong><em>EMNLP 2021</em></strong>.
            </li>


          </ul>
        </div>

      <section id="publications">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Publications</heading></strong>
            </td>
          </table>
        </section>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
                    <img src='images/neurocomputing.png' width="200" class="enlarge">
                  </div>
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>High-Compressed Deepfake Video Detection with Contrastive Spatiotemporal Distillation </papertitle></strong>
                  <br>
                  Yizhe Zhu,
                  <a href="https://scholar.google.com/citations?user=r_cpud8AAAAJ&hl=en">Chunuhi Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  Zihan Rui,
                  Xi Zhou,
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/pii/S0925231223009955">paper</a>
<!--                   /
                  <a href="#">code</a> -->
                  <br>
                  in <em>Neurocomputing</em>, 2023. (<strong>CCF-C</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                      We propose a Contrastive SpatioTemporal Distilling (CSTD) approach that leverages spatial-frequency cues and temporal-contrastive alignment to improve high-compressed deepfake video detection. Our approach employs a two-stage spatiotemporal video encoder to fully exploit spatiotemporal inconsistency information. A fine-grained spatial-frequency distillation module is used to retrieve invariant forgery cues in spatial and frequency domains. 
                      Additionally, a mutual-information temporal-contrastive distillation module is introduced to enhance the temporal correlated information and transfer the temporal structural knowledge from the teacher model to the student model. 
                      We demonstrate the effectiveness and robustness of our method on low-quality high-compressed deepfake videos on public benchmarks.
                    </details>
                </td>
              </tr>

              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
                    <img src='images/All-in-One.png' width="200" class="enlarge">
                  </div>
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=r_cpud8AAAAJ&hl=en">Chunuhi Zhang*</a>,
                  <span style="color: red"><strong>Xin Sun</strong></span>*,
                  <a href="https://scholar.google.com/citations?user=KQ2S01UAAAAJ&hl=en">Li Liu</a>,
                  Yiqian Yang,
                  <a href="https://scholar.google.com/citations?user=cH-m1i8AAAAJ&hl=en">Qiong Liu</a>,
                  Xi Zhou,
                  <a href="https://scholar.google.com/citations?hl=en&user=x_sgJskAAAAJ">Yanfeng Wang</a>,
                  <br>
                  (* means equal contribution)
                  <br>
                  <!-- <a href="https://arxiv.org/abs/2307.03373">paper</a> -->
                  <a href='https://dl.acm.org/doi/10.1145/3581783.3611803'>paper</a>
                  /
                  <a href="https://github.com/983632847/All-in-One">code</a>
                  <br>
                  in <em>ACM International Conference on Multimedia (<strong>ACM MM</strong>)</em>, 2023. (<strong>CCF-A</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We present a simple, compact and effective one-stream framework for VL tracking, namely All-in-One, which learns
                    VL representations from raw visual and language signals end-to-end in a unified transformer backbone.
                    The core insight is to is establish bidirectional information flow between well
                    aligned visual and language signals as early as possible.
                    We also develop a novel multi-modal alignment module incorporating cross-modal and intra-modal alignments to learn more reasonable VL representations.                
                    Extensive experiments on multiple VL tracking benchmarks have demonstrated the effectiveness and generalization of our approach.
                    </details>
                </td>
              </tr>

              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
                    <img src='images/ICANN.png' width="200" class="enlarge">
                  </div>
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Exploiting Multi-modal Fusion for Robust Face Representation Learning with Missing Modality </papertitle></strong>
                  <br>
                  Yizhe Zhu,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  Xi Zhou,
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-44210-0_23">paper</a>
                  <br>
                  in <em>International Conference on Artificial Neural Networks (<strong>ICANN</strong>)</em>, 2023. (<strong>CCF-C</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We propose a multi-modal fusion framework that addresses the problem of uncertain missing modalities in face recognition.
                    Specifically, we first introduce a novel modality-missing loss function based on triplet hard loss to learn individual features for RGB, depth, and thermal modalities. 
                    We then use a central moment discrepancy (CMD) based distance constraint training strategy to learn joint modality-invariant representations. 
                    This approach fully leverages the characteristics of heterogeneous modalities to mitigate the modality gap, resulting in robust multi-modal joint representations.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="CRNet_stop()" onmouseover="CRNet_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='CRNet_image'>
                      <img src='images/CRNet_motivation.png' width="200">
                    </div> -->
                    <img src='images/CRNet_framework.png' width="200" class="enlarge">
                  </div>
<!--                   <script type="text/javascript">
                    function CRNet_start() {
                      document.getElementById('CRNet_image').style.opacity = "1";
                    }

                    function CRNet_stop() {
                      document.getElementById('CRNet_image').style.opacity = "0";
                    }
                    CRNet_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Video Moment Retrieval via Comprehensive Relation-aware Network </papertitle></strong>
                  <br>
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
				          <!-- <a href="#">Yizhe Zhu</a>, -->
                  Yizhe Zhu,
                  <!-- <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=SmlaHBYAAAAJ">Xuan Wang</a>, -->
                  Xuan Wang,
                  Xi Zhou,
                  <!-- &nbsp <em>in TCSVT 2023</em>   -->
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10056330">paper</a>
                  <br>
                  in <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2023. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                  	This manuscript is a substantial <strong>extension of our RaNet in EMNLP2021</strong> with several improvements (<em>i.e.</em> background suppression module, clip-level interaction, and IoU attention mechanism). 
                  	<!--  First, a background suppression module is introduced to reduce the impact of unrelated background clips. 
                  	Second, we model the interactions between clips and sentences, as well as clips and words, to comprehensively capture inter-modality relations. 
                  	Finally, we propose a novel IoU attention mechanism to focus more on the dependencies among highly-correlated video moments. -->
                  	The background suppression module and IoU attention mechanism work together to enhance the hierarchical relations within the model by respectively modulating clip-level and moment-level features. 
                  	The clip-level interaction provides a complementary perspective by capturing localized visual information, resulting in a multi-granular perception of inter-modality information. 
                  	As a result, the harmonious integration of these modules significantly improves the model's ability to model comprehensive relations.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="w2w_stop()" onmouseover="w2w_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='w2w_image'>
                      <img src='images/which_to_where_motivation.png' width="200">
                    </div> -->
                    <img src='images/which_to_where_framework2.png' width="200" class="enlarge">
                  </div>
<!--                   <script type="text/javascript">
                    function w2w_start() {
                      document.getElementById('w2w_image').style.opacity = "1";
                    }

                    function w2w_stop() {
                      document.getElementById('w2w_image').style.opacity = "0";
                    }
                    w2w_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>Efficient Video Grounding with Which-Where Reading Comprehension </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  Xi Zhou,
                  <a href="https://imsg.ac.cn/people/geshiming.html">Shiming Ge</a>,
                  <!-- &nbsp <em>in TCSVT 202</em>   -->
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9771472">paper</a>
                  <br>
                  in <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2022. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We present an efficient framework in a fashion <strong>from which to where</strong> to facilitate video grounding. The core idea is imitating
                    the reading comprehension process to gradually narrow the decision space. The “which” step first roughly selects a candidate area by evaluating 
                    which video segment in the predefined set is closest to the ground truth, while the “where” step aims to precisely regress the temporal boundary of 
                    the selected video segment from the shrunk decision space. Extensive experiments demonstrate the effectiveness of our framework.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="MGPN_stop()" onmouseover="MGPN_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='MGPN_image'>
                      <img src='images/MGPN_motivation.png' width="200">
                    </div> -->
                    <img src='images/MGPN_framework.png' width="200" class="enlarge">
                  </div>
<!--                   <script type="text/javascript">
                    function MGPN_start() {
                      document.getElementById('MGPN_image').style.opacity = "1";
                    }

                    function MGPN_stop() {
                      document.getElementById('MGPN_image').style.opacity = "0";
                    }
                    MGPN_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                      <strong><papertitle>You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos </papertitle></strong>
                  <br>
                  <span style="color: red"><strong>Xin Sun</strong></span>,
                  Xuan Wang,
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=cH-m1i8AAAAJ">Qiong Liu</a>,
                  Xi Zhou,
                  <!-- &nbsp <em>in SIGIR 2022</em>   -->
                  <br>
                  <!-- <a href="https://arxiv.org/abs/2205.12886">paper</a> -->
                  <a href="https://dl.acm.org/doi/10.1145/3477495.3532083">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/MGPN">code</a>
                  <br>
                  in <em>International ACM SIGIR Conference on Research and Development in Information Retrieval (<strong>SIGIR</strong>)</em>, 2022. (<strong>CCF-A</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We formulate moment retrieval task from the perspective of multi-choice reading comprehension and propose a novel 
                    <strong>M</strong>ulti-<strong>G</strong>ranularity <strong>P</strong>erception <strong>N</strong>etwork (<strong>MGPN</strong>) to tackle it. We integrate several human reading strategies 
                    (i.e. passage question reread, enhanced passage question alignment, choice comparison) into our framework 
                    and empower our model to perceive intra-modality and inter-modality information at a multi-granularity level. 
                    Extensive experiments demonstrate the effectiveness and efficiency of our proposed MGPN.
                    </details>
                </td>
              </tr>


              <!-- <tr onmouseout="RaNet_stop()" onmouseover="RaNet_start()"> -->
              <tr style="border-bottom: 1px solid #ddd;">
                <td style="padding:10px;width:35%;vertical-align:top">
                  <div class="one">
<!--                     <div class="two" id='RaNet_image'>
                      <img src='images/RaNet_motivation.png' width="200">
                    </div> -->
                    <img src='images/RaNet_framework.png' width="200" class="enlarge">
                  </div>
<!--                   <script type="text/javascript">
                    function RaNet_start() {
                      document.getElementById('RaNet_image').style.opacity = "1";
                    }

                    function RaNet_stop() {
                      document.getElementById('RaNet_image').style.opacity = "0";
                    }
                    RaNet_stop()
                  </script> -->
                </td>
                <td style="padding:10px;width:65%;vertical-align:top">
                    <strong><papertitle>Relation-aware Video Reading Comprehension for Temporal Language Grounding </papertitle></strong>
                  <br>
                  <a href="https://scholar.google.com/citations?user=sj4FqEgAAAAJ&hl=en">Jialin Gao</a>*,
                  <span style="color: red"><strong>Xin Sun</strong></span>*,
                  <a href="https://scholar.google.com/citations?user=be_ox9QAAAAJ&hl=en">MengMeng Xu</a>,
                  Xi Zhou,
                  <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>,
                  <!-- &nbsp <em>in EMNLP 2021</em>   -->
                  <br>
                  (* means equal contribution)
                  <br>
                  <!-- <a href="https://aclanthology.org/2021.emnlp-main.324.pdf">paper</a> -->
                  <a href="https://aclanthology.org/2021.emnlp-main.324/">paper</a>
                  /
                  <a href="https://github.com/Huntersxsx/RaNet">code</a>
                  <br>
                  in <em>Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>)</em>, 2021. (<strong>CCF-B</strong>)
                  <p></p>
                  <details align="justify">
                    <summary>Details</summary>
                    We propose a novel <strong>R</strong>elation-<strong>a</strong>ware <strong>Net</strong>work (<strong>RaNet</strong>) 
                    to address the problem of temporal language grounding in videos. We propose to interact the visual and textual modalities 
                    in a coarse-and-fine fashion for token-aware and sentence-aware representation of each choice. 
                    Further, a GAT layer is introduced to mine the exhaustive relations between multi-choices for better ranking. 
                    Our model is efficient and outperforms the SOTA methods.
                  </details>
                </td>
              </tr>

          </table>

      <section id="internship">
        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Internship Experience</heading></strong>
            </td>
          </table>
        </section>

        <ul>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/ant.png" alt="WSFG" width="200px" height="64px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2023.02-2023.07]</span> &#129489;&#8205;&#128187; 
                                                   <!-- &nbsp;&nbsp;&nbsp;&nbsp;</em>  -->
                                                   Tiansuan Lab, AntGroup, Shanghai.
                                                    <br>
                                                    <em>Referring Image Segmentation; Multi-Modal Large Language Model</em>

                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/oppo2.png" alt="WSFG" width="200px" height="64px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2021.12-2022.02]</span> &#129489;&#8205;&#128187; 
                                                  <!-- &nbsp;&nbsp;&nbsp;&nbsp;   -->
                                                  Multi-modal Perception Group, OPPO, Shanghai.
                                                  <br>
                                                  <em>Vision Language Pre-training; Human Object Interaction</em>
                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/cloudwalk.png" alt="WSFG" width="200px" height="64px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2020.07-2021.11]</span> &#129489;&#8205;&#128187; 
                                                  <!-- <em>2020.07-2021.11 &nbsp;&nbsp;&nbsp;&nbsp;</em>  -->
                                                  Video Analysis Group, Cloudwalk, Shanghai.
                                                  <br>
                                                  <em>Video Moment Retrieval; Action Recognition</em>
                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          </ul>
          <br />

      <section id="edution">
        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Eduction</heading></strong>
            </td>
          </table>
        </section>

        <ul>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/sjtu.png" alt="WSFG" width="200px" height="64px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2019.09-Now]</span> &#129489;&#8205;&#127891; 
                                                   <!-- &nbsp;&nbsp;&nbsp;&nbsp;</em>  -->
                                                   Shanghai Jiao Tong University (SJTU), Shanghai.
                                                    <br>
                                                    <em>Ph.D. student in Information Engineering</em>

                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/polyu.png" alt="WSFG" width="200px" height="45px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2017.07-2017.08]</span> &#129489;&#8205;&#127891; 
                                                  <!-- &nbsp;&nbsp;&nbsp;&nbsp;   -->
                                                  The Hong Kong Polytechnic University (PolyU), Hong Kong.
                                                  <br>
                                                  <em>Short-term Exchange</em>
                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          <table class="imgtable">
                          <tbody>
                              <tr>
                                  <td>
                                      <img src="images/xjtu.png" alt="WSFG" width="200px" height="55px">
                                      &nbsp;
                                  </td>
                                  <td align="left">
                                      <ul style="list-style-type: none;">
                                          <li>
                                              <p>
                                                  <span style="color: gray" size="6px">[2015.08-2019.06]</span> &#129489;&#8205;&#127891; 
                                                  <!-- <em>2020.07-2021.11 &nbsp;&nbsp;&nbsp;&nbsp;</em>  -->
                                                  Xi'an Jiao Tong University (XJTU), Xi'an.
                                                  <br>
                                                  <em>Bachelor of Engineering in Information Engineering</em>
                                              </p>
                                          </li>
                                      </ul>
                                  </td>
                              </tr>
                          </tbody>
                      </table>
          </ul>
          <br />

          <section id="awards">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Awards</heading></strong>
            </td>
          </table>
        </section>

          <style>
              .scrolling-list {
                max-height: 160px; /* 设置最大高度，超过此高度将显示滚动条 */
                overflow-y: auto; /* 只在需要时显示垂直滚动条 */
                overflow-x: hidden; /* 隐藏水平滚动条 */
              }
          </style>

          <div class="scrolling-list">

          <ul>
          <li><span style="color: gray" size="6px">[2022.11]</span>
            &#128176 Huawei Scholarship. 
          </li>          
          <li><span style="color: gray" size="6px">[2020.05]</span>
            &#127942 Regional Excellence Award in algorithm competition held by ZTE. 
          </li>         
          <li><span style="color: gray" size="6px">[2018.10]</span>
            &#127942 Outstanding Student Model Nomination in Xi'an Jiaotong University. 
          </li>
          <li><span style="color: gray" size="6px">[2018.10]</span>
            &#128176 Special Scholarship of Xi'an Jiaotong University (only 10 places). 
          </li>
           <li><span style="color: gray" size="6px">[2018.08]</span>
            &#129351 First Place in National Undergraduate Electronic Design Contest.
          </li> 
          <li><span style="color: gray" size="6px">[2018.01]</span>
            &#129351 First place in the VR training project held by Shanghai Jiaotong University. 
          </li>
          <li><span style="color: gray" size="6px">[2017.11]</span>
            &#129352 Second Prize for the 11th Shaanxi College Students Mathematics Competition. 
          </li>
          <li><span style="color: gray" size="6px">[2017.10]</span>
            &#128176 National Endeavor Scholarship (top 5%). 
          </li>
          <li><span style="color: gray" size="6px">[2016.12]</span>
            &#129351 First Prize for Shaanxi Division of the Undergraduate Group of the National University Students Mathematical Contest in Modeling. 
          </li>
          <li><span style="color: gray" size="6px">[2016.11]</span>
            &#129351 First Prize for the 8th National College Student Mathematics Competition.
          </li>
          <li><span style="color: gray" size="6px">[2016.10]</span>
            &#128176 PengKang Scholarship (top 5%). 
          </li>
          <li><span style="color: gray" size="6px">[2016.05]</span>
            &#129353 Third Prize for Band C in 2016 National English Competition for College Students. 
          </li>    

        </ul>

      </div>




          </ul>

        <br/>

      <section id="services">
        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong><heading>Professional Services</heading></strong>
            </td>
          </table>
        </section>


        <ul>
          <li>
            <strong>Conference Reviewer:</strong> ACM MM'23
          </li>
           <li>
            <strong>Journal Reviewer:</strong> TCSVT
          </li>         

        </ul>

        <br />
        <br />
         <a href="https://visitorbadge.io/status?path=https%3A%2F%2Fhuntersxsx.github.io%2F"><img src="https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fhuntersxsx.github.io%2F&countColor=%23dce775&style=flat" /></a>

        </td>
      </tr>


  </table>
</body>

<script>
  // 当点击图片时触发放大显示效果
  function enlargeImage() {
    var enlargedImg = document.createElement("div");
    enlargedImg.className = "enlarged-img";
    
    var img = document.createElement("img");
    img.src = this.src;
    enlargedImg.appendChild(img);
    
    enlargedImg.addEventListener("click", function() {
      this.parentNode.removeChild(this);
    });
    
    document.body.appendChild(enlargedImg);
  }
  
  // 获取所有图片元素并添加点击事件
  var images = document.getElementsByTagName("img");
  for (var i = 0; i < images.length; i++) {
    images[i].addEventListener("click", enlargeImage);
  }
</script>

</html>